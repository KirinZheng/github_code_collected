# mmdet/hooks/lock_memory_hook.py
import torch
from mmengine.hooks import Hook
from mmengine.registry import HOOKS  # å¿…é¡»æ‰‹åŠ¨æ³¨å†Œ


@HOOKS.register_module()
class PreheatGPUHook(Hook):
    def __init__(self, gb=20):
        self.gb = gb

    def before_train(self, runner):
        num_bytes = int(self.gb * 1024 ** 3)
        num_elements = num_bytes // 4  # float32
        print(f'[PreheatGPUHook] Preallocating {self.gb:.1f} GB GPU memory temporarily...')
        buf = torch.empty(num_elements, dtype=torch.float32, device='cuda')
        del buf  # ğŸ§  åˆ é™¤å PyTorch CUDA allocator ä¸ä¼šé‡Šæ”¾å›ç³»ç»Ÿï¼Œè€Œæ˜¯ä¿ç•™åœ¨å†…å­˜æ± ä¸­
        torch.cuda.empty_cache()
        print(f'[PreheatGPUHook] Released buffer; CUDA memory pool warmed up.')
